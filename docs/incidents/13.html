
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>October 21 post-incident analysis</title>
    <style>
      
body {
  font-family: Helvetica, sans-serif;
}
a, a:visited {
  color:#03e;
}
table {
  border-collapse: collapse;
}
td {
  border: 1px solid #ddd;
  vertical-align:top;
  text-align:left;
  padding:10px;
  line-height:150%;
}
blockquote {
  font-size:125%;
  padding:0px 20px;
  margin:20px 0px;
  border-left:1px solid #ddd;
}
tr.category td {
  font-weight:bold;
  background-color:#ffd;
}
tr.head {
  font-weight:bold;
}
p.footer {
  font-size:75%;
}

    </style>
  </head>
  <body>
  <h1>
    October 21 post-incident analysis
  </h1>
  
<blockquote>
  &ldquo;Connectivity between these locations was restored in 43 seconds, but this brief outage triggered a chain of events that led to 24 hours and 11 minutes of service degradation.&rdquo;
</blockquote>

<table>
  <tr>
    <td>
      Incident
    </td>
    <td>
      #13 at 
      <a href="../organizations.html#GitHub">GitHub</a> on 
      <a href="../years.html#2018">2018/10/21</a> by Jason Warner (CTO)
    </td>
  </tr>
  <tr>
    <td>
      Full report
    </td>
    <td>
      <a href="https://github.blog/2018-10-30-oct21-post-incident-analysis/">https://github.blog/2018-10-30-oct21-post-incident-analysis/</a>
    </td>
  </tr>

  <tr>
    <td>
      How it happened
    </td>
    <td>
      Routine maintenance work to replace failing network equipment led to 43 seconds of lost connectivity between regional datacenters (ie, a network partition). The cluster management software then elected a new primary in a west coast data center (for multiple clusters), directing all writes from the east coast data center to the west coast data center, leaving some un-replicated writes in both data centers and so the primary could not be failed back over to the east coast data center. The resuling cluster topology was not supported.
    </td>
  </tr>
  <tr>
    <td>
      Architecture
    </td>
    <td>
      Multiple connected regional data centers. MySQL database clusters (storing metadata) each with one primary and dozens of read replicas. Data is sharded across clusters managed using Orchestrator and Raft.
    </td>
  </tr>
  <tr>
    <td>
      Technologies
    </td>
    <td>
      <a href="../technologies.html#MySQL">MySQL</a>, <a href="../technologies.html#Orchestrator">Orchestrator</a>, <a href="../technologies.html#Raft">Raft</a>
    </td>
  </tr>
  <tr>
    <td>
      Root cause
    </td>
    <td>
      Routine maintenance work to replace failing network equipment led to 43 seconds of lost connectivity and a cross-data center topology for clusters.
    </td>
  </tr>
  <tr>
    <td>
      Failure
    </td>
    <td>
      Writes to the ("old") primary nodes were not replicated to the new primary node which also began receiving un-replicated writes. Applications writing from one data center to the other experienced latency and timeouts.
    </td>
  </tr>
  <tr>
    <td>
      Impact
    </td>
    <td>
      24 hours of degraded service, including displaying out of date and inconsistent data, and some features were unavailable.
    </td>
  </tr>
  <tr>
    <td>
      Mitigation
    </td>
    <td>
      Restored data from backups, synchronized replicas from both sites, (logging all conflicting writes for later, manual reconciliation), moved the primary node to the appropriate data center and resume queued jobs.
    </td>
  </tr>
</table>

  </body>
</html>
